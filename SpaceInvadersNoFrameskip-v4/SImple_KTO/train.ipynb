{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_util import make_vec_env as make_vec_env_sb3\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "import imageio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 14 23:40:57 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     On  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  8%   53C    P2              57W / 250W |    526MiB / 11264MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1654141      C   ...iu/miniconda3/envs/myenv/bin/python      276MiB |\n",
      "|    0   N/A  N/A   1677689      C   ...iu/miniconda3/envs/myenv/bin/python      246MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# clean up memory forcefully\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, observations, actions):\n",
    "        self.observations = torch.tensor(observations, dtype=torch.float32,device=device)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.float32,device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.observations[idx]\n",
    "        action = self.actions[idx]\n",
    "        return observation, action\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self,  output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(3136, 512)\n",
    "        self.fc2 = nn.Linear(512, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 4, 84, 84) (100000,) (100000,) (100000,) (100000,) (100000, 4, 84, 84)\n",
      "{'lives': 3, 'episode_frame_number': 34, 'frame_number': 34, 'TimeLimit.truncated': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "env_id=\"SpaceInvadersNoFrameskip-v4\"\n",
    "#env = make_vec_env_sb3(env_id, n_envs=1)\n",
    "observations= np.load(\"../dataset/obs.npy\", allow_pickle=True)\n",
    "actions = np.load(\"../dataset/actions.npy\", allow_pickle=True)\n",
    "rewards = np.load(\"../dataset/rewards.npy\", allow_pickle=True)\n",
    "dones = np.load(\"../dataset/dones.npy\", allow_pickle=True)\n",
    "info = np.load(\"../dataset/info.npy\", allow_pickle=True)\n",
    "next_obs = np.load(\"../dataset/next_obs.npy\", allow_pickle=True)\n",
    "observations=observations.squeeze()\n",
    "actions=actions.squeeze()\n",
    "rewards=rewards.squeeze()\n",
    "dones=dones.squeeze()\n",
    "info=info.squeeze()\n",
    "next_obs=next_obs.squeeze()\n",
    "# observations = observations[:1000]\n",
    "# actions = actions[:1000]\n",
    "print(observations.shape, actions.shape, rewards.shape, dones.shape, info.shape, next_obs.shape)\n",
    "# print(observations[0], actions[0], rewards[0], dones[0], info[0], next_obs[0])\n",
    "print(info[0])\n",
    "env = make_atari_env(env_id, n_envs=1)\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "env = VecTransposeImage(env)            # 确保图像通道在前\n",
    "env = VecFrameStack(env, n_stack=4)     # 堆叠4帧\n",
    "obs = env.reset()\n",
    "print(obs.shape)\n",
    "\n",
    "# print(env.action_space.n)\n",
    "# print(env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydataset(Dataset):\n",
    "    def __init__(self, observations, actions):\n",
    "        self.observations = torch.tensor(observations, dtype=torch.float32)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.observations[idx]\n",
    "        action = self.actions[idx]\n",
    "        return observation, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]:  10%|▉         | 308/3125 [00:05<00:22, 125.16it/s, loss=0.494, positive_reward=51, negative_reward=48.7, margin=2.28, test_score_highest_action=0, test_score_sample_action=0]    /mnt/nfs/work/c98181/miniconda3/envs/atari/lib/python3.9/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "Epoch [1/30]: 100%|██████████| 3125/3125 [00:41<00:00, 74.70it/s, loss=0.946, positive_reward=-17.1, negative_reward=6.16, margin=-23.2, test_score_highest_action=0, test_score_sample_action=7]       \n",
      "Epoch [2/30]: 100%|██████████| 3125/3125 [00:35<00:00, 87.11it/s, loss=0.42, positive_reward=24.4, negative_reward=22.8, margin=1.56, test_score_highest_action=0, test_score_sample_action=6]           \n",
      "Epoch [3/30]: 100%|██████████| 3125/3125 [00:35<00:00, 87.00it/s, loss=0.991, positive_reward=-29.9, negative_reward=8.95, margin=-38.8, test_score_highest_action=0, test_score_sample_action=7]         \n",
      "Epoch [4/30]: 100%|██████████| 3125/3125 [00:38<00:00, 81.02it/s, loss=0.535, positive_reward=34.2, negative_reward=34.9, margin=-.651, test_score_highest_action=0, test_score_sample_action=8]          \n",
      "Epoch [5/30]: 100%|██████████| 3125/3125 [00:36<00:00, 85.06it/s, loss=0.964, positive_reward=-29.3, negative_reward=8.2, margin=-37.5, test_score_highest_action=0, test_score_sample_action=10]          \n",
      "Epoch [6/30]: 100%|██████████| 3125/3125 [00:37<00:00, 84.25it/s, loss=0.429, positive_reward=29.2, negative_reward=26, margin=3.18, test_score_highest_action=0, test_score_sample_action=6]            \n",
      "Epoch [7/30]: 100%|██████████| 3125/3125 [00:35<00:00, 86.86it/s, loss=0.562, positive_reward=-.0401, negative_reward=0.311, margin=-.351, test_score_highest_action=0, test_score_sample_action=2]      \n",
      "Epoch [8/30]: 100%|██████████| 3125/3125 [00:38<00:00, 81.60it/s, loss=0.516, positive_reward=40.8, negative_reward=42.1, margin=-1.35, test_score_highest_action=0, test_score_sample_action=0]          \n",
      "Epoch [9/30]: 100%|██████████| 3125/3125 [00:37<00:00, 84.01it/s, loss=0.589, positive_reward=-.266, negative_reward=0.41, margin=-.676, test_score_highest_action=0, test_score_sample_action=5]         \n",
      "Epoch [10/30]: 100%|██████████| 3125/3125 [00:37<00:00, 83.87it/s, loss=0.523, positive_reward=32.8, negative_reward=34.5, margin=-1.72, test_score_highest_action=0, test_score_sample_action=6]          \n",
      "Epoch [11/30]:  26%|██▌       | 818/3125 [00:09<00:19, 118.49it/s, loss=0.5, positive_reward=29.3, negative_reward=29.3, margin=0, test_score_highest_action=0, test_score_sample_action=0]        "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。\n",
      "\u001b[1;31m請檢閱儲存格中的程式碼，找出失敗的可能原因。\n",
      "\u001b[1;31m如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。\n",
      "\u001b[1;31m如需詳細資料，請檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>。"
     ]
    }
   ],
   "source": [
    "# 导入IPython显示控制模块\n",
    "\n",
    "\n",
    "policy_network = PolicyNetwork(env.action_space.n).to(device)\n",
    "# nn init kaiming\n",
    "torch.nn.init.kaiming_normal_(policy_network.conv1.weight)\n",
    "torch.nn.init.kaiming_normal_(policy_network.conv2.weight)\n",
    "torch.nn.init.kaiming_normal_(policy_network.conv3.weight)\n",
    "torch.nn.init.kaiming_normal_(policy_network.fc1.weight)\n",
    "torch.nn.init.kaiming_normal_(policy_network.fc2.weight)\n",
    "\n",
    "prev = PolicyNetwork(env.action_space.n).to(device)\n",
    "prev.load_state_dict(policy_network.state_dict())\n",
    "prev.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_network.parameters(), lr=1e-4,weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "dataset = mydataset(observations=observations, actions=actions)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "loss_graph=[]\n",
    "eval_rewards2=[]\n",
    "eval_rewards1=[]\n",
    "margin_graph=[]\n",
    "positive_reward_graph=[]\n",
    "negative_reward_graph=[]\n",
    "log_interval = len(observations)//10\n",
    "num_test = 1\n",
    "prev_load_freq = 1000\n",
    "now_log=0\n",
    "now_pos=0\n",
    "test_score1=0\n",
    "test_score2=0\n",
    "env.close()\n",
    "for epoch in range(num_epochs):\n",
    "    policy_network.train()\n",
    "\n",
    "    # Compute the log probabilities of the actions\n",
    "    pbar=tqdm(dataloader,position=0,leave=True)\n",
    "    now_log=0\n",
    "\n",
    "    for step, (obs_batch,act_batch) in enumerate(pbar):\n",
    "        obs_batch = torch.tensor(np.array(obs_batch), dtype=torch.float32).to(device)\n",
    "        act_batch = torch.tensor(np.array(act_batch), dtype=torch.float32).to(device)\n",
    "\n",
    "        now_log+=obs_batch.shape[0]\n",
    "        now_pos+=obs_batch.shape[0]\n",
    "        \n",
    "        logits = policy_network(obs_batch)\n",
    "        model_dist = Categorical(logits=logits)\n",
    "        model_act_sample = model_dist.sample()\n",
    "        policy_chosen_logps = model_dist.log_prob(act_batch)\n",
    "        policy_rejected_logps = model_dist.log_prob(model_act_sample)\n",
    "        with torch.no_grad():\n",
    "            prev_logits = prev(obs_batch)\n",
    "            prev_dist = Categorical(logits=prev_logits)\n",
    "            reference_chosen_logps = prev_dist.log_prob(act_batch)\n",
    "            reference_rejected_logps = prev_dist.log_prob(model_act_sample)\n",
    "        \n",
    "        \n",
    "        beta = 1\n",
    "\n",
    "\n",
    "        # simple version KTO\n",
    "        chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)\n",
    "        rejected_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)\n",
    "\n",
    "        chosen_logratios = (policy_chosen_logps - reference_chosen_logps)\n",
    "        rejected_logratios = (policy_rejected_logps - reference_rejected_logps)\n",
    "\n",
    "        losses = torch.cat((1 - F.sigmoid(beta * (chosen_logratios - rejected_KL)), 1 - F.sigmoid(beta * (chosen_KL - rejected_logratios))), 0)\n",
    "\n",
    "        chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach()\n",
    "        rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach()\n",
    "        loss=losses.mean()\n",
    "\n",
    " \n",
    "        # Optimize the policy\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if (step+1) % prev_load_freq == 0:\n",
    "            prev.load_state_dict(policy_network.state_dict())\n",
    "            prev.eval()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        pbar.set_description((f\"Epoch [{epoch+1}/{num_epochs}]\"))\n",
    "        positive_reward = chosen_logratios.detach().mean().item()\n",
    "        negative_reward = rejected_logratios.detach().mean().item()\n",
    "        margin = positive_reward - negative_reward\n",
    "        loss_graph.append(loss.detach().item())\n",
    "        margin_graph.append(margin)\n",
    "        positive_reward_graph.append(positive_reward)\n",
    "        negative_reward_graph.append(negative_reward)\n",
    "        pbar.set_postfix({\"loss\":loss.detach().item(), \"positive_reward\": positive_reward, \"negative_reward\": negative_reward, \"margin\": positive_reward - negative_reward, \"test_score_highest_action\": test_score1,\"test_score_sample_action\": test_score2})\n",
    "\n",
    "\n",
    "        total_reward=0\n",
    "        if now_log>=log_interval:\n",
    "            env = make_atari_env(env_id, n_envs=1)\n",
    "            env = VecTransposeImage(env)            # 确保图像通道在前\n",
    "            env = VecFrameStack(env, n_stack=4)     # 堆叠4帧\n",
    "            \n",
    "            now_log-=log_interval\n",
    "            \n",
    "            policy_network.eval()  # 切换到评估模式\n",
    "\n",
    "            \n",
    "\n",
    "            total_reward=0\n",
    "            # test the policy and save as the first test as gif\n",
    "            frames = []\n",
    "            saved_frame = False\n",
    "            for _ in range(num_test):\n",
    "                \n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        action = Categorical(logits=(policy_network(state_tensor))).sample().cpu().numpy()\n",
    "                        \n",
    "                    state, reward, done, _ = env.step(action)  # 执行动作\n",
    "                    total_reward += np.sum(reward)\n",
    "                    if not saved_frame:\n",
    "                        frame = env.render(mode=\"rgb_array\")\n",
    "                        frames.append(frame)  # 添加到帧列表中\n",
    "                # 保存为GIF\n",
    "                if not saved_frame:\n",
    "                    saved_frame = True\n",
    "                    image_path = f\"sample_epoch_{epoch+1}_iteration_{now_pos}.gif\"\n",
    "                    imageio.mimsave(\"./gif/\"+image_path, frames)\n",
    "            test_score2 = total_reward/num_test\n",
    "            eval_rewards1.append(total_reward/num_test)\n",
    "            total_reward=0\n",
    "            # test the policy and save as the first test as gif\n",
    "            # frames = []\n",
    "            # saved_frame = False\n",
    "            # for _ in range(num_test):\n",
    "                \n",
    "            #     state = env.reset()\n",
    "            #     done = False\n",
    "            #     while not done:\n",
    "            #         state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            #         with torch.no_grad():\n",
    "            #             logits = policy_network(state_tensor)\n",
    "            #             probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            #             action = probabilities.argmax().item()                        \n",
    "            #         state, reward, done, _ = env.step([action])  # 执行动作\n",
    "            #         total_reward += np.sum(reward)\n",
    "            #         if not saved_frame:\n",
    "            #             frame = env.render(mode=\"rgb_array\")\n",
    "            #             frames.append(frame)  # 添加到帧列表中\n",
    "            #     # 保存为GIF\n",
    "            #     if not saved_frame:\n",
    "            #         saved_frame = True\n",
    "            #         image_path = f\"highest_epoch_{epoch+1}_iteration_{now_pos}.gif\"\n",
    "            #         imageio.mimsave(\"./gif/\"+image_path, frames)\n",
    "            # test_score1 = total_reward/num_test\n",
    "            env.close()\n",
    "            # print(total_reward/num_test)\n",
    "            \n",
    "            eval_rewards2.append(total_reward/num_test)\n",
    "            policy_network.train()  # 切换回训练模式\n",
    "\n",
    "\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(policy_network.state_dict(), \"./model.pth\")\n",
    "# draw the loss graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_graph)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Iteration\")\n",
    "plt.savefig(\"./loss.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# draw the reward graph\n",
    "\n",
    "plt.plot(eval_rewards2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward1 vs Iteration\")\n",
    "plt.savefig(\"./highest_reward.png\")\n",
    "plt.show()\n",
    "plt.plot(eval_rewards1)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Reward1\")\n",
    "plt.title(\"Reward1 vs Iteration\")\n",
    "plt.savefig(\"./sample_reward.png\")\n",
    "plt.show()\n",
    "\n",
    "# drqw the margin graph\n",
    "plt.plot(margin_graph)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Margin\")\n",
    "plt.title(\"Margin vs Iteration\")\n",
    "plt.savefig(\"./margin.png\")\n",
    "plt.show()\n",
    "\n",
    "# draw the positive reward graph\n",
    "plt.plot(positive_reward_graph)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Positive Reward\")\n",
    "plt.title(\"Positive Reward vs Iteration\")\n",
    "plt.savefig(\"./positive_reward.png\")\n",
    "plt.show()\n",
    "\n",
    "# draw the negative reward graph\n",
    "plt.plot(negative_reward_graph)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Negative Reward\")\n",
    "plt.title(\"Negative Reward vs Iteration\")\n",
    "\n",
    "plt.savefig(\"./negative_reward.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread(\"/mnt/nfs/work/albertliu/RL/CartPole-v1/dataset/\"+env_id+\"_28000_rewards.png\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
