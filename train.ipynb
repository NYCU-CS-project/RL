{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_util import make_vec_env as make_vec_env_sb3\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "import imageio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  2 17:55:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     On  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   44C    P8              16W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnvidia-smi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# clean up memory forcefully\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m device\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# clean up memory forcefully\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, observations, actions):\n",
    "        self.observations = torch.tensor(observations, dtype=torch.float32,device=device)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.float32,device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.observations[idx]\n",
    "        action = self.actions[idx]\n",
    "        return observation, action\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.norm1 = nn.LayerNorm(32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.norm2 = nn.LayerNorm(32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.fc1(x)))\n",
    "        x = F.relu(self.norm2(self.fc2(x)))\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 4) (28000,) (28000,) (28000,) (28000,) (28000, 4)\n",
      "[-0.0455423  -0.00493775  0.01715994  0.02859923] 0 1.0 False {'TimeLimit.truncated': False} [-0.04564106 -0.20030153  0.01773192  0.32664654]\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# env = make_vec_env(env_id, n_envs=1)\n",
    "# read /mnt/nfs/work/c98181/RL/dataset/CartPole-v1...npy\n",
    "# np.save(\"/mnt/nfs/work/c98181/RL/dataset/\"+env_id+f\"_28000_obs.npy\", obs_list)\n",
    "# np.save(\"/mnt/nfs/work/c98181/RL/dataset/\"+env_id+\"_28000_actions.npy\", actions_list)\n",
    "# np.save(\"/mnt/nfs/work/c98181/RL/dataset/\"+env_id+\"_28000_rewards.npy\", rewards_list)\n",
    "# np.save(\"/mnt/nfs/work/c98181/RL/dataset/\"+env_id+\"_28000_dones.npy\", dones_list)\n",
    "# np.save(\"/mnt/nfs/work/c98181/RL/dataset/\"+env_id+\"_28000_info.npy\", info_list)\n",
    "# np.save(\"/mnt/nfs/work/c98181/RL/dataset/\"+env_id+\"_28000_next_obs.npy\", next_obs_list)\n",
    "observations= np.load(\"/mnt/nfs/work/c98181/RL/dataset/CartPole-v1_28000_obs.npy\", allow_pickle=True)\n",
    "actions = np.load(\"/mnt/nfs/work/c98181/RL/dataset/CartPole-v1_28000_actions.npy\", allow_pickle=True)\n",
    "rewards = np.load(\"/mnt/nfs/work/c98181/RL/dataset/CartPole-v1_28000_rewards.npy\", allow_pickle=True)\n",
    "dones = np.load(\"/mnt/nfs/work/c98181/RL/dataset/CartPole-v1_28000_dones.npy\", allow_pickle=True)\n",
    "info = np.load(\"/mnt/nfs/work/c98181/RL/dataset/CartPole-v1_28000_info.npy\", allow_pickle=True)\n",
    "next_obs = np.load(\"/mnt/nfs/work/c98181/RL/dataset/CartPole-v1_28000_next_obs.npy\", allow_pickle=True)\n",
    "observations=observations.squeeze()\n",
    "actions=actions.squeeze()\n",
    "rewards=rewards.squeeze()\n",
    "dones=dones.squeeze()\n",
    "info=info.squeeze()\n",
    "next_obs=next_obs.squeeze()\n",
    "\n",
    "# observations = observations[:1000]\n",
    "# actions = actions[:1000]\n",
    "print(observations.shape, actions.shape, rewards.shape, dones.shape, info.shape, next_obs.shape)\n",
    "print(observations[0], actions[0], rewards[0], dones[0], info[0], next_obs[0])\n",
    "\n",
    "# print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydataset(Dataset):\n",
    "    def __init__(self, observations, actions):\n",
    "        self.observations = torch.tensor(observations, dtype=torch.float32).to(device)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.observations[idx]\n",
    "        action = self.actions[idx]\n",
    "        return observation, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import gymnasium as gym\n",
    "# from imitation.policies.serialize import load_policy\n",
    "# from imitation.util.util import make_vec_env\n",
    "# from imitation.data.wrappers import RolloutInfoWrapper\n",
    "# env = make_vec_env(\n",
    "#     \"seals:seals/CartPole-v0\",\n",
    "#     rng=np.random.default_rng(),\n",
    "#     post_wrappers=[\n",
    "#         lambda env, _: RolloutInfoWrapper(env)\n",
    "#     ],  # needed for computing rollouts later\n",
    "# )\n",
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# # reward, _ = evaluate_policy(expert, env, 10)\n",
    "# print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:   1%|▏         | 23/1750 [00:00<00:31, 55.13it/s, loss=0.692, positive_reward=0.00362, negative_reward=0.000447, margin=0.00318]/tmp/ipykernel_1291810/3251528753.py:117: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025847094/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  state_tensor = torch.tensor([state], dtype=torch.float32).to(device)\n",
      "Epoch [1/1]:   3%|▎         | 58/1750 [00:19<08:50,  3.19it/s, loss=0.692, positive_reward=0.000578, negative_reward=-.000952, margin=0.00153] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77.6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:   4%|▍         | 73/1750 [00:48<21:58,  1.27it/s, loss=0.697, positive_reward=-.00177, negative_reward=0.00553, margin=-.0073]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117.9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:   6%|▋         | 110/1750 [01:18<19:45,  1.38it/s, loss=0.686, positive_reward=0.0103, negative_reward=-.00513, margin=0.0154]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:   8%|▊         | 135/1750 [01:47<22:40,  1.19it/s, loss=0.694, positive_reward=0.00206, negative_reward=0.00355, margin=-.00149]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[126.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  10%|█         | 178/1750 [02:08<14:57,  1.75it/s, loss=0.691, positive_reward=0.000997, negative_reward=-.0038, margin=0.00479]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96.2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  11%|█▏        | 200/1750 [02:44<22:40,  1.14it/s, loss=0.704, positive_reward=-.00761, negative_reward=0.0135, margin=-.0211]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156.9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  14%|█▎        | 237/1750 [03:26<22:58,  1.10it/s, loss=0.692, positive_reward=0.00246, negative_reward=-.000408, margin=0.00287]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  15%|█▌        | 269/1750 [04:15<26:19,  1.07s/it, loss=0.693, positive_reward=-6.9e-6, negative_reward=-6.9e-6, margin=0]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  17%|█▋        | 297/1750 [04:57<27:36,  1.14s/it, loss=0.695, positive_reward=0.00384, negative_reward=0.00811, margin=-.00426]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  19%|█▉        | 330/1750 [05:36<25:01,  1.06s/it, loss=0.696, positive_reward=-.00188, negative_reward=0.00406, margin=-.00594]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178.7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  21%|██        | 363/1750 [06:23<25:23,  1.10s/it, loss=0.692, positive_reward=0.000954, negative_reward=-.00177, margin=0.00272]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[214.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  22%|██▏       | 392/1750 [07:21<30:18,  1.34s/it, loss=0.693, positive_reward=-.000775, negative_reward=-.00074, margin=-3.52e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[257.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  24%|██▍       | 424/1750 [08:18<31:06,  1.41s/it, loss=0.691, positive_reward=0.00569, negative_reward=0.00149, margin=0.0042]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[267.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  26%|██▌       | 454/1750 [09:23<34:15,  1.59s/it, loss=0.702, positive_reward=-.00202, negative_reward=0.0156, margin=-.0176]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[301.6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  28%|██▊       | 486/1750 [10:21<32:02,  1.52s/it, loss=0.686, positive_reward=0.00979, negative_reward=-.00529, margin=0.0151]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[253.7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  29%|██▉       | 516/1750 [11:28<34:19,  1.67s/it, loss=0.693, positive_reward=-.000174, negative_reward=-.000114, margin=-6.04e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[306.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  31%|███▏      | 549/1750 [12:24<30:30,  1.52s/it, loss=0.692, positive_reward=0.000127, negative_reward=-.00169, margin=0.00182]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:  33%|███▎      | 581/1750 [13:26<30:06,  1.55s/it, loss=0.697, positive_reward=-.00409, negative_reward=0.00365, margin=-.00774]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291.6]\n"
     ]
    }
   ],
   "source": [
    "policy_network = PolicyNetwork(\n",
    "    4, 2).to(device)\n",
    "\n",
    "nn.init.kaiming_normal_(policy_network.fc1.weight)\n",
    "nn.init.kaiming_normal_(policy_network.fc2.weight)\n",
    "nn.init.kaiming_normal_(policy_network.fc3.weight)\n",
    "prev = PolicyNetwork(\n",
    "    4, 2).to(device)\n",
    "prev.load_state_dict(policy_network.state_dict())\n",
    "prev.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_network.parameters(), lr=0.0017601048183920826,weight_decay=2.350251568550711e-5)\n",
    "env=make_vec_env_sb3(env_id, n_envs=1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "batch_size = 16\n",
    "\n",
    "dataset = mydataset(observations=observations, actions=actions)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "loss_graph=[]\n",
    "eval_rewards=[]\n",
    "margin_graph=[]\n",
    "positive_reward_graph=[]\n",
    "negative_reward_graph=[]\n",
    "log_interval = 500\n",
    "now_log=0\n",
    "now_pos=0\n",
    "for epoch in range(num_epochs):\n",
    "    policy_network.train()\n",
    "\n",
    "    # Compute the log probabilities of the actions\n",
    "    pbar=tqdm(dataloader,position=0,leave=True)\n",
    "    now_log=0\n",
    "\n",
    "    for step, (obs_batch,act_batch) in enumerate(pbar):\n",
    "        # obs_batch = torch.tensor(np.array(obs_batch), dtype=torch.float32).to(device)\n",
    "        # act_batch = torch.tensor(np.array(act_batch), dtype=torch.float32).to(device)\n",
    "\n",
    "        now_log+=obs_batch.shape[0]\n",
    "        now_pos+=obs_batch.shape[0]\n",
    "        \n",
    "        logits = policy_network(obs_batch)\n",
    "        model_dist = Categorical(logits=logits)\n",
    "        model_act_sample = model_dist.sample()\n",
    "        policy_chosen_logps = model_dist.log_prob(act_batch)\n",
    "        policy_rejected_logps = model_dist.log_prob(model_act_sample)\n",
    "        with torch.no_grad():\n",
    "            prev_dist = Categorical(logits=prev(obs_batch))\n",
    "            reference_chosen_logps = prev_dist.log_prob(act_batch)\n",
    "            reference_rejected_logps = prev_dist.log_prob(model_act_sample)\n",
    "        # print all logps\n",
    "        # print(\"policy_chosen_logps\",policy_chosen_logps.detach().mean().item())\n",
    "        # print(\"policy_rejected_logps\",policy_rejected_logps.detach().mean().item())\n",
    "        # print(\"reference_chosen_logps\",reference_chosen_logps.detach().mean().item())\n",
    "        # print(\"reference_rejected_logps\",reference_rejected_logps.detach().mean().item())\n",
    "\n",
    "        pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "        ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "        # print(\"pi_logratios\",pi_logratios.detach().mean().item())\n",
    "        # print(\"ref_logratios\",ref_logratios.detach().mean().item())\n",
    "        # chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)\n",
    "        # reject_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)\n",
    "\n",
    "        logits = pi_logratios - ref_logratios\n",
    "        # print(\"logits\",logits.detach().mean().item())\n",
    "\n",
    "        chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "        reject_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "\n",
    "        # logits = chosen_logratios - reject_logratios\n",
    "\n",
    "        if epoch <= -1:\n",
    "          loss = - (policy_chosen_logps).mean()\n",
    "        else:\n",
    "          beta = 1\n",
    "          losses = (-F.logsigmoid(beta * logits))\n",
    "          loss = losses.mean()\n",
    "        #   losses = torch.cat((1 - F.logsigmoid(beta * (chosen_logratios - reject_KL)), 1 - F.logsigmoid(beta * (chosen_KL - reject_logratios))), 0)\n",
    "        # Optimize the policy\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        prev.load_state_dict(policy_network.state_dict())\n",
    "        prev.eval()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        pbar.set_description((f\"Epoch [{epoch+1}/{num_epochs}]\"))\n",
    "        positive_reward = chosen_logratios.detach().mean().item()\n",
    "        negative_reward = reject_logratios.detach().mean().item()\n",
    "        margin = positive_reward - negative_reward\n",
    "        loss_graph.append(loss.detach().item())\n",
    "        margin_graph.append(margin)\n",
    "        positive_reward_graph.append(positive_reward)\n",
    "        negative_reward_graph.append(negative_reward)\n",
    "        pbar.set_postfix({\"loss\":loss.detach().item(), \"positive_reward\": positive_reward, \"negative_reward\": negative_reward, \"margin\": positive_reward - negative_reward})\n",
    "\n",
    "\n",
    "        total_reward=0\n",
    "        num_test=10\n",
    "        if now_log>=log_interval:\n",
    "            now_log-=log_interval\n",
    "            eval_rewards.append(eval)\n",
    "            policy_network.eval()  # 切换到评估模式\n",
    "\n",
    "            \n",
    "\n",
    "            total_reward=0\n",
    "            # test the policy and save as gif\n",
    "            frames = []\n",
    "            for _ in range(num_test):\n",
    "                state = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    state_tensor = torch.tensor([state], dtype=torch.float32).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        action = Categorical(logits=(policy_network(state_tensor))).sample().cpu().numpy()[0]\n",
    "                        \n",
    "                    state, reward, done, _ = env.step(action)  # 执行动作\n",
    "                    total_reward += reward\n",
    "                    frame = env.render(mode=\"rgb_array\")  # 获取当前环境的图像\n",
    "                    frames.append(frame)  # 添加到帧列表中\n",
    "                # 保存为GIF\n",
    "            image_path = f\"cartpole_epoch_{now_pos}.gif\"\n",
    "            imageio.mimsave(\"/mnt/nfs/work/c98181/RL/result/\"+env_id+\"/\"+image_path, frames)  # duration控制帧切换的速度\n",
    "            print(total_reward/num_test)\n",
    "            eval_rewards.append(total_reward)\n",
    "            policy_network.train()  # 切换回训练模式\n",
    "\n",
    "\n",
    "         \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the loss graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_graph)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.show()\n",
    "\n",
    "# draw the reward graph\n",
    "plt.plot(eval_rewards)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward vs Iteration\")\n",
    "plt.show()\n",
    "\n",
    "# drqw the margin graph\n",
    "plt.plot(margin_graph)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Margin\")\n",
    "plt.title(\"Margin vs Iteration\")\n",
    "plt.show()\n",
    "\n",
    "# draw the positive reward graph\n",
    "plt.plot(positive_reward_graph)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Positive Reward\")\n",
    "plt.title(\"Positive Reward vs Iteration\")\n",
    "plt.show()\n",
    "\n",
    "# draw the negative reward graph\n",
    "plt.plot(negative_reward_graph)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Negative Reward\")\n",
    "plt.title(\"Negative Reward vs Iteration\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
