{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "import imageio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from stable_baselines3.dqn.policies import MlpPolicy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2428"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  2 01:06:48 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     On  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  8%   52C    P2              57W / 250W |    250MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    928510      C   ...81/miniconda3/envs/atari/bin/python      246MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# clean up memory forcefully\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, observations, actions):\n",
    "        self.observations = torch.tensor(observations, dtype=torch.float32,device=device)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.float32,device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.observations[idx]\n",
    "        action = self.actions[idx]\n",
    "        return observation, action\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 4) (1000000,)\n",
      "[-0.03013654  0.21001437 -0.04612853 -0.34675622] 1\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# env = make_vec_env(env_id, n_envs=1)\n",
    "# read /mnt/nfs/work/c98181/RL/dataset/CartPole-v1...npy\n",
    "observations= np.load(\"/mnt/nfs/work/c98181/RL/dataset/CartPole-v1_1M_obs.npy\", allow_pickle=True)\n",
    "actions = np.load(\"/mnt/nfs/work/c98181/RL/dataset/CartPole-v1_1M_actions.npy\", allow_pickle=True)\n",
    "observations=observations.squeeze()\n",
    "actions=actions.squeeze()\n",
    "# observations = observations[:1000]\n",
    "# actions = actions[:1000]\n",
    "print(observations.shape, actions.shape)\n",
    "print(observations[0], actions[0])\n",
    "\n",
    "# print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydataset(Dataset):\n",
    "    def __init__(self, observations, actions):\n",
    "        self.observations = torch.tensor(observations, dtype=torch.float32).to(device)\n",
    "        self.actions = torch.tensor(actions, dtype=torch.float32).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.observations[idx]\n",
    "        action = self.actions[idx]\n",
    "        return observation, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network = PolicyNetwork(\n",
    "    4, 2).to(device)\n",
    "\n",
    "prev = PolicyNetwork(\n",
    "    4, 2).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_network.parameters(), lr=(1e-3))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "batch_size = 512\n",
    "\n",
    "dataset = mydataset(observations=observations, actions=actions)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "graph=[]\n",
    "eval_rewards=[]\n",
    "for epoch in range(num_epochs):\n",
    "    policy_network.train()\n",
    "\n",
    "    # Compute the log probabilities of the actions\n",
    "    pbar=tqdm(dataloader,position=0,leave=True)\n",
    "    loss_record=[]\n",
    "    # step=0\n",
    "    for step, (obs_batch,act_batch) in enumerate(pbar):\n",
    "        # obs_batch = torch.tensor(np.array(obs_batch), dtype=torch.float32).to(device)\n",
    "        # act_batch = torch.tensor(np.array(act_batch), dtype=torch.float32).to(device)\n",
    "        if step % 100 == 0:\n",
    "          prev.load_state_dict(policy_network.state_dict())\n",
    "          prev.eval()\n",
    "        # Get the log probabilities of the actions\n",
    "        \n",
    "        softmax = policy_network(obs_batch)\n",
    "        # print(softmax.shape, act_batch.shape)\n",
    "        model_act_sample = Categorical(softmax).sample()\n",
    "        policy_chosen_logps = Categorical(softmax).log_prob(act_batch)\n",
    "        policy_rejected_logps = Categorical(softmax).log_prob(model_act_sample)\n",
    "        with torch.no_grad():\n",
    "            reference_chosen_logps = Categorical(prev(obs_batch)).log_prob(act_batch)\n",
    "            reference_rejected_logps = Categorical(prev(obs_batch)).log_prob(model_act_sample)\n",
    "          \n",
    "\n",
    "        pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "        ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "        # chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)\n",
    "        # reject_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)\n",
    "\n",
    "        logits = pi_logratios - ref_logratios\n",
    "\n",
    "        chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "        reject_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "\n",
    "        # logits = chosen_logratios - reject_logratios\n",
    "\n",
    "        if epoch <= 10:\n",
    "          loss = - (policy_chosen_logps).mean()\n",
    "        else:\n",
    "          beta = 1\n",
    "          losses = (\n",
    "                  -F.logsigmoid(beta * logits)\n",
    "              )\n",
    "          # loss = losses.mean()\n",
    "        #   losses = torch.cat((1 - F.logsigmoid(beta * (chosen_logratios - reject_KL)), 1 - F.logsigmoid(beta * (chosen_KL - reject_logratios))), 0)\n",
    "          loss = losses.mean()\n",
    "        # Optimize the policy\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description((f\"Epoch [{epoch+1}/{num_epochs}]\"))\n",
    "        positive_reward = chosen_logratios.detach().mean().item()\n",
    "        negative_reward = reject_logratios.detach().mean().item()\n",
    "        pbar.set_postfix({\"loss\":loss.detach().item(), \"positive_reward\": positive_reward, \"negative_reward\": negative_reward, \"margin\": positive_reward - negative_reward})\n",
    "\n",
    "\n",
    "        #scheduler\n",
    "\n",
    "        loss_record.append(loss.detach().item())\n",
    "    scheduler.step()\n",
    "    graph.append(sum(loss_record)/len(loss_record))\n",
    "\n",
    "    policy_network.eval()  # 切换到评估模式\n",
    "\n",
    "    env = make_vec_env(env_id, n_envs=1)\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward=0\n",
    "    # test the policy and save as gif\n",
    "    frames = []\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor([state], dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            action = Categorical(policy_network(state_tensor)).sample().cpu().numpy()[0]\n",
    "            \n",
    "        state, reward, done, _ = env.step(action)  # 执行动作\n",
    "        total_reward += reward\n",
    "        frame = env.render(mode=\"rgb_array\")  # 获取当前环境的图像\n",
    "        frames.append(frame)  # 添加到帧列表中\n",
    "\n",
    "    # 保存为GIF\n",
    "    image_path = f\"cartpole_epoch_{epoch+1}.gif\"\n",
    "    imageio.mimsave(\"/mnt/nfs/work/c98181/RL/result/\"+env_id+\"/\"+image_path, frames, duration=0.04)  # duration控制帧切换的速度\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    print(total_reward)\n",
    "    eval_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the loss graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(graph)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.show()\n",
    "\n",
    "# draw the reward graph\n",
    "plt.plot(eval_rewards)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward vs Epoch\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]: 100%|██████████| 879/879 [00:06<00:00, 129.02it/s, loss=0.196]\n",
      "Epoch [2/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.74it/s, loss=0.16] \n",
      "Epoch [3/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.88it/s, loss=0.142]\n",
      "Epoch [4/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.46it/s, loss=0.137]\n",
      "Epoch [5/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.11it/s, loss=0.148]\n",
      "Epoch [6/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.72it/s, loss=0.134]\n",
      "Epoch [7/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.12it/s, loss=0.135]\n",
      "Epoch [8/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.73it/s, loss=0.131] \n",
      "Epoch [9/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.81it/s, loss=0.145] \n",
      "Epoch [10/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.68it/s, loss=0.132] \n",
      "Epoch [11/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.00it/s, loss=0.144] \n",
      "Epoch [12/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.78it/s, loss=0.105] \n",
      "Epoch [13/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.49it/s, loss=0.13]  \n",
      "Epoch [14/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.70it/s, loss=0.129] \n",
      "Epoch [15/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.98it/s, loss=0.124] \n",
      "Epoch [16/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.18it/s, loss=0.126] \n",
      "Epoch [17/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.92it/s, loss=0.127] \n",
      "Epoch [18/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.26it/s, loss=0.119] \n",
      "Epoch [19/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.65it/s, loss=0.125] \n",
      "Epoch [20/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.98it/s, loss=0.128] \n",
      "Epoch [21/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.77it/s, loss=0.12]  \n",
      "Epoch [22/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.82it/s, loss=0.133] \n",
      "Epoch [23/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.02it/s, loss=0.131] \n",
      "Epoch [24/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.59it/s, loss=0.113] \n",
      "Epoch [25/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.36it/s, loss=0.114] \n",
      "Epoch [26/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.33it/s, loss=0.12]  \n",
      "Epoch [27/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.15it/s, loss=0.121] \n",
      "Epoch [28/1000]: 100%|██████████| 879/879 [00:07<00:00, 116.80it/s, loss=0.112] \n",
      "Epoch [29/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.25it/s, loss=0.128] \n",
      "Epoch [30/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.28it/s, loss=0.126] \n",
      "Epoch [31/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.87it/s, loss=0.105] \n",
      "Epoch [32/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.94it/s, loss=0.106] \n",
      "Epoch [33/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.63it/s, loss=0.128] \n",
      "Epoch [34/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.63it/s, loss=0.126] \n",
      "Epoch [35/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.46it/s, loss=0.112] \n",
      "Epoch [36/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.47it/s, loss=0.116] \n",
      "Epoch [37/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.76it/s, loss=0.0882]\n",
      "Epoch [38/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.21it/s, loss=0.119] \n",
      "Epoch [39/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.20it/s, loss=0.107] \n",
      "Epoch [40/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.38it/s, loss=0.107] \n",
      "Epoch [41/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.79it/s, loss=0.108] \n",
      "Epoch [42/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.34it/s, loss=0.109] \n",
      "Epoch [43/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.48it/s, loss=0.116] \n",
      "Epoch [44/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.86it/s, loss=0.11]  \n",
      "Epoch [45/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.28it/s, loss=0.114] \n",
      "Epoch [46/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.42it/s, loss=0.0869]\n",
      "Epoch [47/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.43it/s, loss=0.0957]\n",
      "Epoch [48/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.72it/s, loss=0.123] \n",
      "Epoch [49/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.72it/s, loss=0.121] \n",
      "Epoch [50/1000]: 100%|██████████| 879/879 [00:07<00:00, 119.65it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [51/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.53it/s, loss=0.101] \n",
      "Epoch [52/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.79it/s, loss=0.0954]\n",
      "Epoch [53/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.74it/s, loss=0.106] \n",
      "Epoch [54/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.67it/s, loss=0.124] \n",
      "Epoch [55/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.66it/s, loss=0.126] \n",
      "Epoch [56/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.40it/s, loss=0.107] \n",
      "Epoch [57/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.51it/s, loss=0.118] \n",
      "Epoch [58/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.65it/s, loss=0.104] \n",
      "Epoch [59/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.10it/s, loss=0.125] \n",
      "Epoch [60/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.37it/s, loss=0.108] \n",
      "Epoch [61/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.63it/s, loss=0.119] \n",
      "Epoch [62/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.52it/s, loss=0.0893]\n",
      "Epoch [63/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.51it/s, loss=0.136] \n",
      "Epoch [64/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.85it/s, loss=0.0858]\n",
      "Epoch [65/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.43it/s, loss=0.122] \n",
      "Epoch [66/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.76it/s, loss=0.101] \n",
      "Epoch [67/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.42it/s, loss=0.1]   \n",
      "Epoch [68/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.57it/s, loss=0.122] \n",
      "Epoch [69/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.97it/s, loss=0.0808]\n",
      "Epoch [70/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.15it/s, loss=0.105] \n",
      "Epoch [71/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.81it/s, loss=0.0982]\n",
      "Epoch [72/1000]: 100%|██████████| 879/879 [00:07<00:00, 119.02it/s, loss=0.106] \n",
      "Epoch [73/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.58it/s, loss=0.113] \n",
      "Epoch [74/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.37it/s, loss=0.0931]\n",
      "Epoch [75/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.54it/s, loss=0.0906]\n",
      "Epoch [76/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.46it/s, loss=0.1]   \n",
      "Epoch [77/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.43it/s, loss=0.104] \n",
      "Epoch [78/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.58it/s, loss=0.101] \n",
      "Epoch [79/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.44it/s, loss=0.121] \n",
      "Epoch [80/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.66it/s, loss=0.107] \n",
      "Epoch [81/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.30it/s, loss=0.127] \n",
      "Epoch [82/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.93it/s, loss=0.0922]\n",
      "Epoch [83/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.33it/s, loss=0.109] \n",
      "Epoch [84/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.37it/s, loss=0.107] \n",
      "Epoch [85/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.71it/s, loss=0.101] \n",
      "Epoch [86/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.12it/s, loss=0.0976]\n",
      "Epoch [87/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.98it/s, loss=0.104] \n",
      "Epoch [88/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.80it/s, loss=0.101] \n",
      "Epoch [89/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.52it/s, loss=0.0959]\n",
      "Epoch [90/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.33it/s, loss=0.118] \n",
      "Epoch [91/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.70it/s, loss=0.0818]\n",
      "Epoch [92/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.01it/s, loss=0.109] \n",
      "Epoch [93/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.45it/s, loss=0.0906]\n",
      "Epoch [94/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.72it/s, loss=0.101] \n",
      "Epoch [95/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.68it/s, loss=0.112] \n",
      "Epoch [96/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.11it/s, loss=0.11]  \n",
      "Epoch [97/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.35it/s, loss=0.0889]\n",
      "Epoch [98/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.24it/s, loss=0.11]  \n",
      "Epoch [99/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.13it/s, loss=0.111] \n",
      "Epoch [100/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.62it/s, loss=0.0953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [101/1000]: 100%|██████████| 879/879 [00:06<00:00, 128.30it/s, loss=0.101] \n",
      "Epoch [102/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.67it/s, loss=0.0953]\n",
      "Epoch [103/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.14it/s, loss=0.0843]\n",
      "Epoch [104/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.33it/s, loss=0.0873]\n",
      "Epoch [105/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.87it/s, loss=0.0979]\n",
      "Epoch [106/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.20it/s, loss=0.118] \n",
      "Epoch [107/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.92it/s, loss=0.0951]\n",
      "Epoch [108/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.48it/s, loss=0.109] \n",
      "Epoch [109/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.18it/s, loss=0.0924]\n",
      "Epoch [110/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.56it/s, loss=0.117] \n",
      "Epoch [111/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.07it/s, loss=0.0775]\n",
      "Epoch [112/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.09it/s, loss=0.107] \n",
      "Epoch [113/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.30it/s, loss=0.118] \n",
      "Epoch [114/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.75it/s, loss=0.115] \n",
      "Epoch [115/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.62it/s, loss=0.0998]\n",
      "Epoch [116/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.76it/s, loss=0.0928]\n",
      "Epoch [117/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.51it/s, loss=0.0886]\n",
      "Epoch [118/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.75it/s, loss=0.0921]\n",
      "Epoch [119/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.17it/s, loss=0.0915]\n",
      "Epoch [120/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.64it/s, loss=0.0951]\n",
      "Epoch [121/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.04it/s, loss=0.0955]\n",
      "Epoch [122/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.96it/s, loss=0.0923]\n",
      "Epoch [123/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.23it/s, loss=0.13]  \n",
      "Epoch [124/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.41it/s, loss=0.0984]\n",
      "Epoch [125/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.64it/s, loss=0.135] \n",
      "Epoch [126/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.91it/s, loss=0.116] \n",
      "Epoch [127/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.88it/s, loss=0.0999]\n",
      "Epoch [128/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.58it/s, loss=0.112] \n",
      "Epoch [129/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.91it/s, loss=0.0781]\n",
      "Epoch [130/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.07it/s, loss=0.0916]\n",
      "Epoch [131/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.16it/s, loss=0.125] \n",
      "Epoch [132/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.69it/s, loss=0.118] \n",
      "Epoch [133/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.40it/s, loss=0.0953]\n",
      "Epoch [134/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.41it/s, loss=0.091] \n",
      "Epoch [135/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.13it/s, loss=0.108] \n",
      "Epoch [136/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.78it/s, loss=0.103] \n",
      "Epoch [137/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.46it/s, loss=0.101] \n",
      "Epoch [138/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.25it/s, loss=0.108] \n",
      "Epoch [139/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.92it/s, loss=0.0947]\n",
      "Epoch [140/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.22it/s, loss=0.0958]\n",
      "Epoch [141/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.13it/s, loss=0.106] \n",
      "Epoch [142/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.06it/s, loss=0.0994]\n",
      "Epoch [143/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.42it/s, loss=0.0962]\n",
      "Epoch [144/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.65it/s, loss=0.118] \n",
      "Epoch [145/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.36it/s, loss=0.104] \n",
      "Epoch [146/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.72it/s, loss=0.101] \n",
      "Epoch [147/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.50it/s, loss=0.103] \n",
      "Epoch [148/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.76it/s, loss=0.0891]\n",
      "Epoch [149/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.86it/s, loss=0.112] \n",
      "Epoch [150/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.05it/s, loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [151/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.61it/s, loss=0.109] \n",
      "Epoch [152/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.99it/s, loss=0.0988]\n",
      "Epoch [153/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.73it/s, loss=0.11]  \n",
      "Epoch [154/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.27it/s, loss=0.102] \n",
      "Epoch [155/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.90it/s, loss=0.0813]\n",
      "Epoch [156/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.68it/s, loss=0.104] \n",
      "Epoch [157/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.86it/s, loss=0.0748]\n",
      "Epoch [158/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.00it/s, loss=0.111] \n",
      "Epoch [159/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.76it/s, loss=0.117] \n",
      "Epoch [160/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.48it/s, loss=0.109] \n",
      "Epoch [161/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.49it/s, loss=0.0931]\n",
      "Epoch [162/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.43it/s, loss=0.106] \n",
      "Epoch [163/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.73it/s, loss=0.121] \n",
      "Epoch [164/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.02it/s, loss=0.0893]\n",
      "Epoch [165/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.57it/s, loss=0.1]   \n",
      "Epoch [166/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.18it/s, loss=0.0958]\n",
      "Epoch [167/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.24it/s, loss=0.0891]\n",
      "Epoch [168/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.54it/s, loss=0.0984]\n",
      "Epoch [169/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.68it/s, loss=0.123] \n",
      "Epoch [170/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.58it/s, loss=0.101] \n",
      "Epoch [171/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.15it/s, loss=0.0876]\n",
      "Epoch [172/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.91it/s, loss=0.0975]\n",
      "Epoch [173/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.18it/s, loss=0.0979]\n",
      "Epoch [174/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.70it/s, loss=0.0934]\n",
      "Epoch [175/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.58it/s, loss=0.0982]\n",
      "Epoch [176/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.48it/s, loss=0.0886]\n",
      "Epoch [177/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.80it/s, loss=0.101] \n",
      "Epoch [178/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.55it/s, loss=0.102] \n",
      "Epoch [179/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.05it/s, loss=0.0959]\n",
      "Epoch [180/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.06it/s, loss=0.103] \n",
      "Epoch [181/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.57it/s, loss=0.0897]\n",
      "Epoch [182/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.22it/s, loss=0.0971]\n",
      "Epoch [183/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.53it/s, loss=0.117] \n",
      "Epoch [184/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.49it/s, loss=0.0897]\n",
      "Epoch [185/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.11it/s, loss=0.0988]\n",
      "Epoch [186/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.56it/s, loss=0.121] \n",
      "Epoch [187/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.96it/s, loss=0.0916]\n",
      "Epoch [188/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.20it/s, loss=0.124] \n",
      "Epoch [189/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.76it/s, loss=0.109] \n",
      "Epoch [190/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.00it/s, loss=0.093] \n",
      "Epoch [191/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.70it/s, loss=0.105] \n",
      "Epoch [192/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.09it/s, loss=0.128] \n",
      "Epoch [193/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.02it/s, loss=0.104] \n",
      "Epoch [194/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.73it/s, loss=0.0906]\n",
      "Epoch [195/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.64it/s, loss=0.118] \n",
      "Epoch [196/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.19it/s, loss=0.0863]\n",
      "Epoch [197/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.01it/s, loss=0.0991]\n",
      "Epoch [198/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.46it/s, loss=0.0934]\n",
      "Epoch [199/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.47it/s, loss=0.109] \n",
      "Epoch [200/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.41it/s, loss=0.11]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [201/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.07it/s, loss=0.0979]\n",
      "Epoch [202/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.02it/s, loss=0.106] \n",
      "Epoch [203/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.12it/s, loss=0.0962]\n",
      "Epoch [204/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.13it/s, loss=0.108] \n",
      "Epoch [205/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.71it/s, loss=0.121] \n",
      "Epoch [206/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.14it/s, loss=0.117] \n",
      "Epoch [207/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.72it/s, loss=0.102] \n",
      "Epoch [208/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.78it/s, loss=0.0811]\n",
      "Epoch [209/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.38it/s, loss=0.0722]\n",
      "Epoch [210/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.66it/s, loss=0.109] \n",
      "Epoch [211/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.00it/s, loss=0.105] \n",
      "Epoch [212/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.89it/s, loss=0.094] \n",
      "Epoch [213/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.78it/s, loss=0.102] \n",
      "Epoch [214/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.10it/s, loss=0.104] \n",
      "Epoch [215/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.24it/s, loss=0.126] \n",
      "Epoch [216/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.25it/s, loss=0.0954]\n",
      "Epoch [217/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.73it/s, loss=0.111] \n",
      "Epoch [218/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.38it/s, loss=0.115] \n",
      "Epoch [219/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.57it/s, loss=0.11]  \n",
      "Epoch [220/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.29it/s, loss=0.0917]\n",
      "Epoch [221/1000]: 100%|██████████| 879/879 [00:06<00:00, 129.37it/s, loss=0.109] \n",
      "Epoch [222/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.00it/s, loss=0.0899]\n",
      "Epoch [223/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.66it/s, loss=0.0877]\n",
      "Epoch [224/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.79it/s, loss=0.105] \n",
      "Epoch [225/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.53it/s, loss=0.108] \n",
      "Epoch [226/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.55it/s, loss=0.0993]\n",
      "Epoch [227/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.56it/s, loss=0.0897]\n",
      "Epoch [228/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.40it/s, loss=0.104] \n",
      "Epoch [229/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.16it/s, loss=0.0935]\n",
      "Epoch [230/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.12it/s, loss=0.114] \n",
      "Epoch [231/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.29it/s, loss=0.105] \n",
      "Epoch [232/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.57it/s, loss=0.107] \n",
      "Epoch [233/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.95it/s, loss=0.103] \n",
      "Epoch [234/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.63it/s, loss=0.103] \n",
      "Epoch [235/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.08it/s, loss=0.0937]\n",
      "Epoch [236/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.77it/s, loss=0.0913]\n",
      "Epoch [237/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.21it/s, loss=0.106] \n",
      "Epoch [238/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.66it/s, loss=0.103] \n",
      "Epoch [239/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.00it/s, loss=0.0911]\n",
      "Epoch [240/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.42it/s, loss=0.0863]\n",
      "Epoch [241/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.70it/s, loss=0.0912]\n",
      "Epoch [242/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.44it/s, loss=0.106] \n",
      "Epoch [243/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.71it/s, loss=0.103] \n",
      "Epoch [244/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.83it/s, loss=0.0957]\n",
      "Epoch [245/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.97it/s, loss=0.0977]\n",
      "Epoch [246/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.15it/s, loss=0.0902]\n",
      "Epoch [247/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.50it/s, loss=0.105] \n",
      "Epoch [248/1000]: 100%|██████████| 879/879 [00:07<00:00, 119.66it/s, loss=0.0989]\n",
      "Epoch [249/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.07it/s, loss=0.0986]\n",
      "Epoch [250/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.19it/s, loss=0.0952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [251/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.18it/s, loss=0.103] \n",
      "Epoch [252/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.43it/s, loss=0.102] \n",
      "Epoch [253/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.76it/s, loss=0.107] \n",
      "Epoch [254/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.93it/s, loss=0.0991]\n",
      "Epoch [255/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.85it/s, loss=0.107] \n",
      "Epoch [256/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.92it/s, loss=0.093] \n",
      "Epoch [257/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.78it/s, loss=0.104] \n",
      "Epoch [258/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.89it/s, loss=0.0936]\n",
      "Epoch [259/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.03it/s, loss=0.0759]\n",
      "Epoch [260/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.18it/s, loss=0.0794]\n",
      "Epoch [261/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.96it/s, loss=0.0937]\n",
      "Epoch [262/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.38it/s, loss=0.0907]\n",
      "Epoch [263/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.02it/s, loss=0.0835]\n",
      "Epoch [264/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.96it/s, loss=0.123] \n",
      "Epoch [265/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.02it/s, loss=0.106] \n",
      "Epoch [266/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.12it/s, loss=0.099] \n",
      "Epoch [267/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.06it/s, loss=0.107] \n",
      "Epoch [268/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.80it/s, loss=0.0837]\n",
      "Epoch [269/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.77it/s, loss=0.1]   \n",
      "Epoch [270/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.04it/s, loss=0.0834]\n",
      "Epoch [271/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.79it/s, loss=0.108] \n",
      "Epoch [272/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.75it/s, loss=0.104] \n",
      "Epoch [273/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.47it/s, loss=0.0907]\n",
      "Epoch [274/1000]: 100%|██████████| 879/879 [00:06<00:00, 128.63it/s, loss=0.117] \n",
      "Epoch [275/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.52it/s, loss=0.0953]\n",
      "Epoch [276/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.27it/s, loss=0.0982]\n",
      "Epoch [277/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.87it/s, loss=0.0966]\n",
      "Epoch [278/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.69it/s, loss=0.0903]\n",
      "Epoch [279/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.97it/s, loss=0.0806]\n",
      "Epoch [280/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.78it/s, loss=0.0995]\n",
      "Epoch [281/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.04it/s, loss=0.106] \n",
      "Epoch [282/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.37it/s, loss=0.109] \n",
      "Epoch [283/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.03it/s, loss=0.109] \n",
      "Epoch [284/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.12it/s, loss=0.0882]\n",
      "Epoch [285/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.62it/s, loss=0.104] \n",
      "Epoch [286/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.33it/s, loss=0.0912]\n",
      "Epoch [287/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.36it/s, loss=0.0899]\n",
      "Epoch [288/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.28it/s, loss=0.0936]\n",
      "Epoch [289/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.63it/s, loss=0.0934]\n",
      "Epoch [290/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.15it/s, loss=0.109] \n",
      "Epoch [291/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.43it/s, loss=0.09]  \n",
      "Epoch [292/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.50it/s, loss=0.103] \n",
      "Epoch [293/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.03it/s, loss=0.0859]\n",
      "Epoch [294/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.83it/s, loss=0.0953]\n",
      "Epoch [295/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.21it/s, loss=0.0902]\n",
      "Epoch [296/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.17it/s, loss=0.105] \n",
      "Epoch [297/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.62it/s, loss=0.0979]\n",
      "Epoch [298/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.39it/s, loss=0.108] \n",
      "Epoch [299/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.19it/s, loss=0.109] \n",
      "Epoch [300/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.78it/s, loss=0.116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [301/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.38it/s, loss=0.0979]\n",
      "Epoch [302/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.44it/s, loss=0.101] \n",
      "Epoch [303/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.48it/s, loss=0.101] \n",
      "Epoch [304/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.90it/s, loss=0.109] \n",
      "Epoch [305/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.45it/s, loss=0.115] \n",
      "Epoch [306/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.80it/s, loss=0.123] \n",
      "Epoch [307/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.46it/s, loss=0.109] \n",
      "Epoch [308/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.79it/s, loss=0.101] \n",
      "Epoch [309/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.98it/s, loss=0.105] \n",
      "Epoch [310/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.67it/s, loss=0.0818]\n",
      "Epoch [311/1000]: 100%|██████████| 879/879 [00:06<00:00, 131.49it/s, loss=0.108] \n",
      "Epoch [312/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.18it/s, loss=0.0991]\n",
      "Epoch [313/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.19it/s, loss=0.105] \n",
      "Epoch [314/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.06it/s, loss=0.0968]\n",
      "Epoch [315/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.76it/s, loss=0.101] \n",
      "Epoch [316/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.07it/s, loss=0.118] \n",
      "Epoch [317/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.97it/s, loss=0.114] \n",
      "Epoch [318/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.59it/s, loss=0.0923]\n",
      "Epoch [319/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.62it/s, loss=0.102] \n",
      "Epoch [320/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.13it/s, loss=0.105] \n",
      "Epoch [321/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.42it/s, loss=0.101] \n",
      "Epoch [322/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.96it/s, loss=0.0915]\n",
      "Epoch [323/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.34it/s, loss=0.105] \n",
      "Epoch [324/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.61it/s, loss=0.0983]\n",
      "Epoch [325/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.70it/s, loss=0.0991]\n",
      "Epoch [326/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.24it/s, loss=0.111] \n",
      "Epoch [327/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.10it/s, loss=0.0903]\n",
      "Epoch [328/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.78it/s, loss=0.0894]\n",
      "Epoch [329/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.09it/s, loss=0.115] \n",
      "Epoch [330/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.54it/s, loss=0.11]  \n",
      "Epoch [331/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.34it/s, loss=0.114] \n",
      "Epoch [332/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.89it/s, loss=0.094] \n",
      "Epoch [333/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.54it/s, loss=0.115] \n",
      "Epoch [334/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.36it/s, loss=0.0906]\n",
      "Epoch [335/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.60it/s, loss=0.089] \n",
      "Epoch [336/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.49it/s, loss=0.104] \n",
      "Epoch [337/1000]: 100%|██████████| 879/879 [00:06<00:00, 128.15it/s, loss=0.0978]\n",
      "Epoch [338/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.50it/s, loss=0.0944]\n",
      "Epoch [339/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.59it/s, loss=0.106] \n",
      "Epoch [340/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.70it/s, loss=0.0881]\n",
      "Epoch [341/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.13it/s, loss=0.0933]\n",
      "Epoch [342/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.14it/s, loss=0.0889]\n",
      "Epoch [343/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.20it/s, loss=0.134] \n",
      "Epoch [344/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.46it/s, loss=0.0949]\n",
      "Epoch [345/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.52it/s, loss=0.108] \n",
      "Epoch [346/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.07it/s, loss=0.0823]\n",
      "Epoch [347/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.73it/s, loss=0.118] \n",
      "Epoch [348/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.08it/s, loss=0.0978]\n",
      "Epoch [349/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.45it/s, loss=0.109] \n",
      "Epoch [350/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.48it/s, loss=0.0768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [351/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.62it/s, loss=0.117] \n",
      "Epoch [352/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.48it/s, loss=0.083] \n",
      "Epoch [353/1000]: 100%|██████████| 879/879 [00:06<00:00, 131.44it/s, loss=0.0876]\n",
      "Epoch [354/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.61it/s, loss=0.118] \n",
      "Epoch [355/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.31it/s, loss=0.0889]\n",
      "Epoch [356/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.72it/s, loss=0.101] \n",
      "Epoch [357/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.15it/s, loss=0.101] \n",
      "Epoch [358/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.33it/s, loss=0.108] \n",
      "Epoch [359/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.04it/s, loss=0.127] \n",
      "Epoch [360/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.74it/s, loss=0.113] \n",
      "Epoch [361/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.26it/s, loss=0.0899]\n",
      "Epoch [362/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.62it/s, loss=0.0975]\n",
      "Epoch [363/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.93it/s, loss=0.105] \n",
      "Epoch [364/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.02it/s, loss=0.0738]\n",
      "Epoch [365/1000]: 100%|██████████| 879/879 [00:06<00:00, 130.54it/s, loss=0.111] \n",
      "Epoch [366/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.40it/s, loss=0.118] \n",
      "Epoch [367/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.42it/s, loss=0.1]   \n",
      "Epoch [368/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.65it/s, loss=0.0907]\n",
      "Epoch [369/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.00it/s, loss=0.0891]\n",
      "Epoch [370/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.43it/s, loss=0.0937]\n",
      "Epoch [371/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.84it/s, loss=0.12]  \n",
      "Epoch [372/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.31it/s, loss=0.0841]\n",
      "Epoch [373/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.45it/s, loss=0.109] \n",
      "Epoch [374/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.26it/s, loss=0.11]  \n",
      "Epoch [375/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.53it/s, loss=0.0894]\n",
      "Epoch [376/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.28it/s, loss=0.0873]\n",
      "Epoch [377/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.56it/s, loss=0.0961]\n",
      "Epoch [378/1000]: 100%|██████████| 879/879 [00:06<00:00, 130.44it/s, loss=0.0889]\n",
      "Epoch [379/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.02it/s, loss=0.109] \n",
      "Epoch [380/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.11it/s, loss=0.11]  \n",
      "Epoch [381/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.24it/s, loss=0.109] \n",
      "Epoch [382/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.72it/s, loss=0.11]  \n",
      "Epoch [383/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.45it/s, loss=0.0931]\n",
      "Epoch [384/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.37it/s, loss=0.114] \n",
      "Epoch [385/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.13it/s, loss=0.117] \n",
      "Epoch [386/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.52it/s, loss=0.0879]\n",
      "Epoch [387/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.24it/s, loss=0.106] \n",
      "Epoch [388/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.73it/s, loss=0.112] \n",
      "Epoch [389/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.80it/s, loss=0.0984]\n",
      "Epoch [390/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.10it/s, loss=0.105] \n",
      "Epoch [391/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.55it/s, loss=0.1]   \n",
      "Epoch [392/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.39it/s, loss=0.0914]\n",
      "Epoch [393/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.32it/s, loss=0.108] \n",
      "Epoch [394/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.40it/s, loss=0.0973]\n",
      "Epoch [395/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.24it/s, loss=0.0809]\n",
      "Epoch [396/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.14it/s, loss=0.0969]\n",
      "Epoch [397/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.76it/s, loss=0.091] \n",
      "Epoch [398/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.96it/s, loss=0.115] \n",
      "Epoch [399/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.17it/s, loss=0.0965]\n",
      "Epoch [400/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.82it/s, loss=0.0943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [401/1000]: 100%|██████████| 879/879 [00:06<00:00, 129.59it/s, loss=0.114] \n",
      "Epoch [402/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.43it/s, loss=0.0954]\n",
      "Epoch [403/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.54it/s, loss=0.0887]\n",
      "Epoch [404/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.08it/s, loss=0.117] \n",
      "Epoch [405/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.26it/s, loss=0.0698]\n",
      "Epoch [406/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.56it/s, loss=0.0901]\n",
      "Epoch [407/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.19it/s, loss=0.104] \n",
      "Epoch [408/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.86it/s, loss=0.0868]\n",
      "Epoch [409/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.74it/s, loss=0.0941]\n",
      "Epoch [410/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.72it/s, loss=0.107] \n",
      "Epoch [411/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.59it/s, loss=0.0881]\n",
      "Epoch [412/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.59it/s, loss=0.102] \n",
      "Epoch [413/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.95it/s, loss=0.103] \n",
      "Epoch [414/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.78it/s, loss=0.113] \n",
      "Epoch [415/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.53it/s, loss=0.107] \n",
      "Epoch [416/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.92it/s, loss=0.102] \n",
      "Epoch [417/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.13it/s, loss=0.0962]\n",
      "Epoch [418/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.25it/s, loss=0.0898]\n",
      "Epoch [419/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.89it/s, loss=0.0918]\n",
      "Epoch [420/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.93it/s, loss=0.109] \n",
      "Epoch [421/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.28it/s, loss=0.107] \n",
      "Epoch [422/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.58it/s, loss=0.0927]\n",
      "Epoch [423/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.82it/s, loss=0.0914]\n",
      "Epoch [424/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.57it/s, loss=0.085] \n",
      "Epoch [425/1000]: 100%|██████████| 879/879 [00:07<00:00, 120.03it/s, loss=0.113] \n",
      "Epoch [426/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.96it/s, loss=0.0845]\n",
      "Epoch [427/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.37it/s, loss=0.119] \n",
      "Epoch [428/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.13it/s, loss=0.104] \n",
      "Epoch [429/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.18it/s, loss=0.0897]\n",
      "Epoch [430/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.15it/s, loss=0.0944]\n",
      "Epoch [431/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.35it/s, loss=0.0939]\n",
      "Epoch [432/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.10it/s, loss=0.0978]\n",
      "Epoch [433/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.05it/s, loss=0.0798]\n",
      "Epoch [434/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.70it/s, loss=0.123] \n",
      "Epoch [435/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.56it/s, loss=0.0844]\n",
      "Epoch [436/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.09it/s, loss=0.0829]\n",
      "Epoch [437/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.89it/s, loss=0.0989]\n",
      "Epoch [438/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.74it/s, loss=0.0902]\n",
      "Epoch [439/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.77it/s, loss=0.0945]\n",
      "Epoch [440/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.36it/s, loss=0.12]  \n",
      "Epoch [441/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.04it/s, loss=0.117] \n",
      "Epoch [442/1000]: 100%|██████████| 879/879 [00:06<00:00, 130.26it/s, loss=0.0844]\n",
      "Epoch [443/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.52it/s, loss=0.12]  \n",
      "Epoch [444/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.17it/s, loss=0.0851]\n",
      "Epoch [445/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.57it/s, loss=0.0919]\n",
      "Epoch [446/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.29it/s, loss=0.11]  \n",
      "Epoch [447/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.31it/s, loss=0.0982]\n",
      "Epoch [448/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.07it/s, loss=0.0893]\n",
      "Epoch [449/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.43it/s, loss=0.0966]\n",
      "Epoch [450/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.85it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [451/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.12it/s, loss=0.104] \n",
      "Epoch [452/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.29it/s, loss=0.103] \n",
      "Epoch [453/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.31it/s, loss=0.0873]\n",
      "Epoch [454/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.52it/s, loss=0.101] \n",
      "Epoch [455/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.79it/s, loss=0.101] \n",
      "Epoch [456/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.92it/s, loss=0.0854]\n",
      "Epoch [457/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.04it/s, loss=0.0884]\n",
      "Epoch [458/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.72it/s, loss=0.0885]\n",
      "Epoch [459/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.09it/s, loss=0.108] \n",
      "Epoch [460/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.44it/s, loss=0.0966]\n",
      "Epoch [461/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.82it/s, loss=0.0925]\n",
      "Epoch [462/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.80it/s, loss=0.094] \n",
      "Epoch [463/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.61it/s, loss=0.0879]\n",
      "Epoch [464/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.22it/s, loss=0.0933]\n",
      "Epoch [465/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.64it/s, loss=0.092] \n",
      "Epoch [466/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.29it/s, loss=0.112] \n",
      "Epoch [467/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.56it/s, loss=0.0965]\n",
      "Epoch [468/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.01it/s, loss=0.0914]\n",
      "Epoch [469/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.36it/s, loss=0.102] \n",
      "Epoch [470/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.50it/s, loss=0.0916]\n",
      "Epoch [471/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.93it/s, loss=0.0849]\n",
      "Epoch [472/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.64it/s, loss=0.123] \n",
      "Epoch [473/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.08it/s, loss=0.104] \n",
      "Epoch [474/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.32it/s, loss=0.0902]\n",
      "Epoch [475/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.94it/s, loss=0.119] \n",
      "Epoch [476/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.27it/s, loss=0.108] \n",
      "Epoch [477/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.02it/s, loss=0.0984]\n",
      "Epoch [478/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.04it/s, loss=0.108] \n",
      "Epoch [479/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.08it/s, loss=0.102] \n",
      "Epoch [480/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.05it/s, loss=0.0948]\n",
      "Epoch [481/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.89it/s, loss=0.0946]\n",
      "Epoch [482/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.33it/s, loss=0.0946]\n",
      "Epoch [483/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.17it/s, loss=0.0833]\n",
      "Epoch [484/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.95it/s, loss=0.121] \n",
      "Epoch [485/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.75it/s, loss=0.0896]\n",
      "Epoch [486/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.00it/s, loss=0.106] \n",
      "Epoch [487/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.03it/s, loss=0.0915]\n",
      "Epoch [488/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.05it/s, loss=0.0936]\n",
      "Epoch [489/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.08it/s, loss=0.0866]\n",
      "Epoch [490/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.49it/s, loss=0.0916]\n",
      "Epoch [491/1000]: 100%|██████████| 879/879 [00:07<00:00, 119.24it/s, loss=0.0977]\n",
      "Epoch [492/1000]: 100%|██████████| 879/879 [00:07<00:00, 118.94it/s, loss=0.0871]\n",
      "Epoch [493/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.76it/s, loss=0.0946]\n",
      "Epoch [494/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.84it/s, loss=0.105] \n",
      "Epoch [495/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.51it/s, loss=0.0855]\n",
      "Epoch [496/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.29it/s, loss=0.0959]\n",
      "Epoch [497/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.89it/s, loss=0.0908]\n",
      "Epoch [498/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.01it/s, loss=0.101] \n",
      "Epoch [499/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.42it/s, loss=0.0743]\n",
      "Epoch [500/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.72it/s, loss=0.113] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [501/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.33it/s, loss=0.116] \n",
      "Epoch [502/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.64it/s, loss=0.0994]\n",
      "Epoch [503/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.56it/s, loss=0.112] \n",
      "Epoch [504/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.44it/s, loss=0.1]   \n",
      "Epoch [505/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.58it/s, loss=0.093] \n",
      "Epoch [506/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.81it/s, loss=0.101] \n",
      "Epoch [507/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.24it/s, loss=0.0822]\n",
      "Epoch [508/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.93it/s, loss=0.0891]\n",
      "Epoch [509/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.43it/s, loss=0.091] \n",
      "Epoch [510/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.33it/s, loss=0.1]   \n",
      "Epoch [511/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.43it/s, loss=0.0819]\n",
      "Epoch [512/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.54it/s, loss=0.094] \n",
      "Epoch [513/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.61it/s, loss=0.0838]\n",
      "Epoch [514/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.13it/s, loss=0.0887]\n",
      "Epoch [515/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.89it/s, loss=0.112] \n",
      "Epoch [516/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.68it/s, loss=0.102] \n",
      "Epoch [517/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.17it/s, loss=0.101] \n",
      "Epoch [518/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.44it/s, loss=0.114] \n",
      "Epoch [519/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.88it/s, loss=0.0871]\n",
      "Epoch [520/1000]: 100%|██████████| 879/879 [00:06<00:00, 129.72it/s, loss=0.106] \n",
      "Epoch [521/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.84it/s, loss=0.104] \n",
      "Epoch [522/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.25it/s, loss=0.0969]\n",
      "Epoch [523/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.11it/s, loss=0.0793]\n",
      "Epoch [524/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.38it/s, loss=0.0908]\n",
      "Epoch [525/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.24it/s, loss=0.0979]\n",
      "Epoch [526/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.79it/s, loss=0.0994]\n",
      "Epoch [527/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.53it/s, loss=0.108] \n",
      "Epoch [528/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.57it/s, loss=0.0832]\n",
      "Epoch [529/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.87it/s, loss=0.0895]\n",
      "Epoch [530/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.16it/s, loss=0.0945]\n",
      "Epoch [531/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.29it/s, loss=0.1]   \n",
      "Epoch [532/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.14it/s, loss=0.0963]\n",
      "Epoch [533/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.08it/s, loss=0.0887]\n",
      "Epoch [534/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.58it/s, loss=0.111] \n",
      "Epoch [535/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.31it/s, loss=0.101] \n",
      "Epoch [536/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.58it/s, loss=0.0968]\n",
      "Epoch [537/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.07it/s, loss=0.113] \n",
      "Epoch [538/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.43it/s, loss=0.0927]\n",
      "Epoch [539/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.45it/s, loss=0.109] \n",
      "Epoch [540/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.20it/s, loss=0.101] \n",
      "Epoch [541/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.87it/s, loss=0.106] \n",
      "Epoch [542/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.11it/s, loss=0.105] \n",
      "Epoch [543/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.69it/s, loss=0.0869]\n",
      "Epoch [544/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.80it/s, loss=0.0951]\n",
      "Epoch [545/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.07it/s, loss=0.0957]\n",
      "Epoch [546/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.29it/s, loss=0.101] \n",
      "Epoch [547/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.08it/s, loss=0.0907]\n",
      "Epoch [548/1000]: 100%|██████████| 879/879 [00:06<00:00, 132.81it/s, loss=0.101] \n",
      "Epoch [549/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.35it/s, loss=0.0868]\n",
      "Epoch [550/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.61it/s, loss=0.0829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [551/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.32it/s, loss=0.103] \n",
      "Epoch [552/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.15it/s, loss=0.0972]\n",
      "Epoch [553/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.53it/s, loss=0.0943]\n",
      "Epoch [554/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.15it/s, loss=0.099] \n",
      "Epoch [555/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.44it/s, loss=0.104] \n",
      "Epoch [556/1000]: 100%|██████████| 879/879 [00:06<00:00, 132.43it/s, loss=0.108] \n",
      "Epoch [557/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.97it/s, loss=0.121] \n",
      "Epoch [558/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.72it/s, loss=0.0956]\n",
      "Epoch [559/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.12it/s, loss=0.0899]\n",
      "Epoch [560/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.98it/s, loss=0.0881]\n",
      "Epoch [561/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.49it/s, loss=0.0926]\n",
      "Epoch [562/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.99it/s, loss=0.118] \n",
      "Epoch [563/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.20it/s, loss=0.112] \n",
      "Epoch [564/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.52it/s, loss=0.0844]\n",
      "Epoch [565/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.37it/s, loss=0.089] \n",
      "Epoch [566/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.56it/s, loss=0.11]  \n",
      "Epoch [567/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.83it/s, loss=0.11]  \n",
      "Epoch [568/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.66it/s, loss=0.0932]\n",
      "Epoch [569/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.48it/s, loss=0.0958]\n",
      "Epoch [570/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.98it/s, loss=0.0871]\n",
      "Epoch [571/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.86it/s, loss=0.1]   \n",
      "Epoch [572/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.38it/s, loss=0.107] \n",
      "Epoch [573/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.46it/s, loss=0.0939]\n",
      "Epoch [574/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.95it/s, loss=0.111] \n",
      "Epoch [575/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.27it/s, loss=0.112] \n",
      "Epoch [576/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.99it/s, loss=0.101] \n",
      "Epoch [577/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.73it/s, loss=0.087] \n",
      "Epoch [578/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.10it/s, loss=0.0918]\n",
      "Epoch [579/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.54it/s, loss=0.0935]\n",
      "Epoch [580/1000]: 100%|██████████| 879/879 [00:06<00:00, 131.64it/s, loss=0.103] \n",
      "Epoch [581/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.23it/s, loss=0.0969]\n",
      "Epoch [582/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.77it/s, loss=0.0842]\n",
      "Epoch [583/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.19it/s, loss=0.103] \n",
      "Epoch [584/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.41it/s, loss=0.0871]\n",
      "Epoch [585/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.03it/s, loss=0.0909]\n",
      "Epoch [586/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.34it/s, loss=0.101] \n",
      "Epoch [587/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.05it/s, loss=0.111] \n",
      "Epoch [588/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.18it/s, loss=0.105] \n",
      "Epoch [589/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.67it/s, loss=0.105] \n",
      "Epoch [590/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.41it/s, loss=0.0969]\n",
      "Epoch [591/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.26it/s, loss=0.0811]\n",
      "Epoch [592/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.59it/s, loss=0.097] \n",
      "Epoch [593/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.83it/s, loss=0.113] \n",
      "Epoch [594/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.35it/s, loss=0.101] \n",
      "Epoch [595/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.46it/s, loss=0.0981]\n",
      "Epoch [596/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.11it/s, loss=0.0988]\n",
      "Epoch [597/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.88it/s, loss=0.0964]\n",
      "Epoch [598/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.92it/s, loss=0.0948]\n",
      "Epoch [599/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.09it/s, loss=0.0688]\n",
      "Epoch [600/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.41it/s, loss=0.0937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [601/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.92it/s, loss=0.106] \n",
      "Epoch [602/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.88it/s, loss=0.112] \n",
      "Epoch [603/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.09it/s, loss=0.0936]\n",
      "Epoch [604/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.65it/s, loss=0.089] \n",
      "Epoch [605/1000]: 100%|██████████| 879/879 [00:07<00:00, 115.83it/s, loss=0.0942]\n",
      "Epoch [606/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.32it/s, loss=0.0959]\n",
      "Epoch [607/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.20it/s, loss=0.107] \n",
      "Epoch [608/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.07it/s, loss=0.0908]\n",
      "Epoch [609/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.59it/s, loss=0.0939]\n",
      "Epoch [610/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.39it/s, loss=0.0784]\n",
      "Epoch [611/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.70it/s, loss=0.101] \n",
      "Epoch [612/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.82it/s, loss=0.0873]\n",
      "Epoch [613/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.08it/s, loss=0.0869]\n",
      "Epoch [614/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.76it/s, loss=0.0973]\n",
      "Epoch [615/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.81it/s, loss=0.1]   \n",
      "Epoch [616/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.09it/s, loss=0.0943]\n",
      "Epoch [617/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.58it/s, loss=0.115] \n",
      "Epoch [618/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.48it/s, loss=0.0844]\n",
      "Epoch [619/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.88it/s, loss=0.101] \n",
      "Epoch [620/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.23it/s, loss=0.0942]\n",
      "Epoch [621/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.84it/s, loss=0.108] \n",
      "Epoch [622/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.87it/s, loss=0.104] \n",
      "Epoch [623/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.34it/s, loss=0.106] \n",
      "Epoch [624/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.57it/s, loss=0.0841]\n",
      "Epoch [625/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.88it/s, loss=0.095] \n",
      "Epoch [626/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.99it/s, loss=0.0856]\n",
      "Epoch [627/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.12it/s, loss=0.0926]\n",
      "Epoch [628/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.89it/s, loss=0.0927]\n",
      "Epoch [629/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.82it/s, loss=0.0873]\n",
      "Epoch [630/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.41it/s, loss=0.0977]\n",
      "Epoch [631/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.95it/s, loss=0.0853]\n",
      "Epoch [632/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.30it/s, loss=0.108] \n",
      "Epoch [633/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.86it/s, loss=0.118] \n",
      "Epoch [634/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.47it/s, loss=0.0973]\n",
      "Epoch [635/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.20it/s, loss=0.0938]\n",
      "Epoch [636/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.97it/s, loss=0.0967]\n",
      "Epoch [637/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.66it/s, loss=0.0892]\n",
      "Epoch [638/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.56it/s, loss=0.0957]\n",
      "Epoch [639/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.60it/s, loss=0.0884]\n",
      "Epoch [640/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.25it/s, loss=0.0984]\n",
      "Epoch [641/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.62it/s, loss=0.0898]\n",
      "Epoch [642/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.57it/s, loss=0.101] \n",
      "Epoch [643/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.83it/s, loss=0.0952]\n",
      "Epoch [644/1000]: 100%|██████████| 879/879 [00:06<00:00, 131.76it/s, loss=0.0975]\n",
      "Epoch [645/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.06it/s, loss=0.107] \n",
      "Epoch [646/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.56it/s, loss=0.109] \n",
      "Epoch [647/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.43it/s, loss=0.0959]\n",
      "Epoch [648/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.61it/s, loss=0.106] \n",
      "Epoch [649/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.08it/s, loss=0.0949]\n",
      "Epoch [650/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.59it/s, loss=0.101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [651/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.95it/s, loss=0.0859]\n",
      "Epoch [652/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.38it/s, loss=0.0906]\n",
      "Epoch [653/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.88it/s, loss=0.107] \n",
      "Epoch [654/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.97it/s, loss=0.105] \n",
      "Epoch [655/1000]: 100%|██████████| 879/879 [00:07<00:00, 119.92it/s, loss=0.082] \n",
      "Epoch [656/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.43it/s, loss=0.0892]\n",
      "Epoch [657/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.50it/s, loss=0.0899]\n",
      "Epoch [658/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.97it/s, loss=0.108] \n",
      "Epoch [659/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.95it/s, loss=0.098] \n",
      "Epoch [660/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.66it/s, loss=0.0895]\n",
      "Epoch [661/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.18it/s, loss=0.0959]\n",
      "Epoch [662/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.23it/s, loss=0.0917]\n",
      "Epoch [663/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.39it/s, loss=0.108] \n",
      "Epoch [664/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.31it/s, loss=0.104] \n",
      "Epoch [665/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.81it/s, loss=0.0924]\n",
      "Epoch [666/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.97it/s, loss=0.112] \n",
      "Epoch [667/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.60it/s, loss=0.114] \n",
      "Epoch [668/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.03it/s, loss=0.108] \n",
      "Epoch [669/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.97it/s, loss=0.11]  \n",
      "Epoch [670/1000]: 100%|██████████| 879/879 [00:07<00:00, 119.78it/s, loss=0.0856]\n",
      "Epoch [671/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.89it/s, loss=0.102] \n",
      "Epoch [672/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.62it/s, loss=0.107] \n",
      "Epoch [673/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.32it/s, loss=0.095] \n",
      "Epoch [674/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.03it/s, loss=0.0911]\n",
      "Epoch [675/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.33it/s, loss=0.101] \n",
      "Epoch [676/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.14it/s, loss=0.101] \n",
      "Epoch [677/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.55it/s, loss=0.0945]\n",
      "Epoch [678/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.96it/s, loss=0.0857]\n",
      "Epoch [679/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.60it/s, loss=0.0907]\n",
      "Epoch [680/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.54it/s, loss=0.0933]\n",
      "Epoch [681/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.88it/s, loss=0.0942]\n",
      "Epoch [682/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.34it/s, loss=0.116] \n",
      "Epoch [683/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.41it/s, loss=0.0997]\n",
      "Epoch [684/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.27it/s, loss=0.1]   \n",
      "Epoch [685/1000]: 100%|██████████| 879/879 [00:06<00:00, 126.05it/s, loss=0.111] \n",
      "Epoch [686/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.39it/s, loss=0.112] \n",
      "Epoch [687/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.00it/s, loss=0.0944]\n",
      "Epoch [688/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.37it/s, loss=0.0904]\n",
      "Epoch [689/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.64it/s, loss=0.0886]\n",
      "Epoch [690/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.88it/s, loss=0.103] \n",
      "Epoch [691/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.02it/s, loss=0.0983]\n",
      "Epoch [692/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.70it/s, loss=0.0858]\n",
      "Epoch [693/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.02it/s, loss=0.102] \n",
      "Epoch [694/1000]: 100%|██████████| 879/879 [00:07<00:00, 119.64it/s, loss=0.104] \n",
      "Epoch [695/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.42it/s, loss=0.0868]\n",
      "Epoch [696/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.47it/s, loss=0.104] \n",
      "Epoch [697/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.38it/s, loss=0.0958]\n",
      "Epoch [698/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.66it/s, loss=0.0796]\n",
      "Epoch [699/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.76it/s, loss=0.0907]\n",
      "Epoch [700/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.85it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [701/1000]: 100%|██████████| 879/879 [00:06<00:00, 127.01it/s, loss=0.0841]\n",
      "Epoch [702/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.50it/s, loss=0.0917]\n",
      "Epoch [703/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.57it/s, loss=0.108] \n",
      "Epoch [704/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.49it/s, loss=0.0906]\n",
      "Epoch [705/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.50it/s, loss=0.101] \n",
      "Epoch [706/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.22it/s, loss=0.101] \n",
      "Epoch [707/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.56it/s, loss=0.0957]\n",
      "Epoch [708/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.42it/s, loss=0.0956]\n",
      "Epoch [709/1000]: 100%|██████████| 879/879 [00:07<00:00, 122.10it/s, loss=0.0953]\n",
      "Epoch [710/1000]: 100%|██████████| 879/879 [00:06<00:00, 125.68it/s, loss=0.0917]\n",
      "Epoch [711/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.39it/s, loss=0.102] \n",
      "Epoch [712/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.21it/s, loss=0.0949]\n",
      "Epoch [713/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.11it/s, loss=0.105] \n",
      "Epoch [714/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.15it/s, loss=0.108] \n",
      "Epoch [715/1000]: 100%|██████████| 879/879 [00:07<00:00, 119.81it/s, loss=0.0879]\n",
      "Epoch [716/1000]: 100%|██████████| 879/879 [00:07<00:00, 116.70it/s, loss=0.0923]\n",
      "Epoch [717/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.05it/s, loss=0.0889]\n",
      "Epoch [718/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.01it/s, loss=0.113] \n",
      "Epoch [719/1000]: 100%|██████████| 879/879 [00:07<00:00, 125.18it/s, loss=0.0941]\n",
      "Epoch [720/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.65it/s, loss=0.0922]\n",
      "Epoch [721/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.21it/s, loss=0.112] \n",
      "Epoch [722/1000]: 100%|██████████| 879/879 [00:07<00:00, 123.44it/s, loss=0.107] \n",
      "Epoch [723/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.27it/s, loss=0.0906]\n",
      "Epoch [724/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.62it/s, loss=0.101] \n",
      "Epoch [725/1000]: 100%|██████████| 879/879 [00:07<00:00, 118.86it/s, loss=0.101] \n",
      "Epoch [726/1000]: 100%|██████████| 879/879 [00:07<00:00, 124.46it/s, loss=0.0893]\n",
      "Epoch [727/1000]: 100%|██████████| 879/879 [00:07<00:00, 121.28it/s, loss=0.0978]\n",
      "Epoch [728/1000]: 100%|██████████| 879/879 [00:07<00:00, 110.11it/s, loss=0.107] \n",
      "Epoch [729/1000]: 100%|██████████| 879/879 [00:08<00:00, 99.18it/s, loss=0.102]  \n",
      "Epoch [730/1000]: 100%|██████████| 879/879 [00:08<00:00, 103.13it/s, loss=0.0932]\n",
      "Epoch [731/1000]: 100%|██████████| 879/879 [00:08<00:00, 100.37it/s, loss=0.111] \n",
      "Epoch [732/1000]:  32%|███▏      | 277/879 [00:02<00:06, 96.54it/s, loss=0.0957] "
     ]
    }
   ],
   "source": [
    "# show line number below\n",
    "\n",
    "\n",
    "policy_network = PolicyNetwork(\n",
    "    4, 2).to(device)\n",
    "# nn init kaiming\n",
    "torch.nn.init.kaiming_normal_(policy_network.fc1.weight)\n",
    "torch.nn.init.kaiming_normal_(policy_network.fc2.weight)\n",
    "torch.nn.init.kaiming_normal_(policy_network.fc3.weight)\n",
    "\n",
    "# build training set and validation set\n",
    "train_size = int(0.9 * len(observations))\n",
    "train_observations = observations[:train_size]\n",
    "train_actions = actions[:train_size]\n",
    "val_observations = observations[train_size:]\n",
    "val_actions = actions[train_size:]\n",
    "\n",
    "# build dataset\n",
    "train_dataset = CustomDataset(train_observations, train_actions)\n",
    "val_dataset = CustomDataset(val_observations, val_actions)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_network.parameters(), lr=(5e-4),weight_decay=1e-4,eps=1e-8)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=1)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "batch_size = 1024\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset , batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset , batch_size=batch_size, shuffle=True)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "graph=[]\n",
    "val_losses=[]\n",
    "eval_rewards=[]\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    policy_network.train()\n",
    "\n",
    "    # Compute the log probabilities of the actions\n",
    "    pbar=tqdm(train_dataloader,position=0,leave=True)\n",
    "    loss_record=[]\n",
    "\n",
    "    for step, (obs_batch,act_batch) in enumerate(pbar):\n",
    "\n",
    "        \n",
    "        logits = policy_network(obs_batch)\n",
    "        # cross entropy\n",
    "        loss=loss_func(logits, act_batch.long())\n",
    "        \n",
    "        # Optimize the policy\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description((f\"Epoch [{epoch+1}/{num_epochs}]\"))\n",
    "        pbar.set_postfix({\"loss\":loss.detach().item()})\n",
    "\n",
    "\n",
    "        #scheduler\n",
    "\n",
    "        loss_record.append(loss.detach().item())\n",
    "    scheduler.step()\n",
    "    graph.append(sum(loss_record)/len(loss_record))\n",
    "    with torch.no_grad():\n",
    "        # validation_loss\n",
    "        val_loss_record=[]\n",
    "        for step, (obs_batch,act_batch) in enumerate(val_dataloader):\n",
    "            logits = policy_network(obs_batch)\n",
    "            loss = loss_func(logits, act_batch.long())\n",
    "            val_loss_record.append(loss.detach().item())\n",
    "        val_loss = sum(val_loss_record)/len(val_loss_record)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    policy_network.eval()  # 切换到评估模式\n",
    "\n",
    "    env = make_vec_env(env_id, n_envs=1)\n",
    "    # test ten times\n",
    "    total_reward=0\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # test the policy and save as gif\n",
    "        frames = []\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor([state], dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred= policy_network(state_tensor)\n",
    "                action = torch.argmax(pred).cpu().numpy()\n",
    "                # print(action)\n",
    "                \n",
    "            state, reward, done, _ = env.step([action])  # 执行动作\n",
    "            total_reward += reward\n",
    "            frame = env.render(mode=\"rgb_array\")  # 获取当前环境的图像\n",
    "            frames.append(frame)  # 添加到帧列表中\n",
    "\n",
    "    # 保存为GIF\n",
    "    image_path = f\"cartpole_epoch_{epoch+1}.gif\"\n",
    "    imageio.mimsave(\"/mnt/nfs/work/c98181/RL/result/\"+env_id+\"/gif/\"+image_path, frames, duration=0.04)  # duration控制帧切换的速度\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    # print(total_reward/10, val_loss)\n",
    "    \n",
    "    eval_rewards.append(total_reward/10)\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(\"saving model\")\n",
    "        torch.save(policy_network.state_dict(), f\"/mnt/nfs/work/c98181/RL/result/{env_id}/model/policy_network_epoch_{epoch+1}.pth\")\n",
    "        # plot the loss graph\n",
    "        plt.plot(graph)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss vs Epoch\")\n",
    "        plt.savefig(f\"/mnt/nfs/work/c98181/RL/result/{env_id}/graph/loss_epoch_{epoch+1}.png\")\n",
    "        plt.close()\n",
    "        # plot the reward graph\n",
    "        plt.plot(eval_rewards)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Reward vs Epoch\")\n",
    "        plt.savefig(f\"/mnt/nfs/work/c98181/RL/result/{env_id}/graph/reward_epoch_{epoch+1}.png\")\n",
    "        plt.close()\n",
    "        # plot the val loss graph\n",
    "        plt.plot(val_losses)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Val Loss\")\n",
    "        plt.title(\"Val Loss vs Epoch\")\n",
    "        plt.savefig(f\"/mnt/nfs/work/c98181/RL/result/{env_id}/graph/val_loss_epoch_{epoch+1}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the loss graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(graph)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.show()\n",
    "\n",
    "# draw the reward graph\n",
    "plt.plot(eval_rewards)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward vs Epoch\")\n",
    "plt.show()\n",
    "# draw the validation loss graph\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss vs Epoch\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pretrain(self, dataset, n_epochs=10, learning_rate=1e-4,\n",
    "#                  adam_epsilon=1e-8, val_interval=None):\n",
    "#         \"\"\"\n",
    "#         Pretrain a model using behavior cloning:\n",
    "#         supervised learning given an expert dataset.\n",
    "\n",
    "#         NOTE: only Box and Discrete spaces are supported for now.\n",
    "\n",
    "#         :param dataset: (ExpertDataset) Dataset manager\n",
    "#         :param n_epochs: (int) Number of iterations on the training set\n",
    "#         :param learning_rate: (float) Learning rate\n",
    "#         :param adam_epsilon: (float) the epsilon value for the adam optimizer\n",
    "#         :param val_interval: (int) Report training and validation losses every n epochs.\n",
    "#             By default, every 10th of the maximum number of epochs.\n",
    "#         :return: (BaseRLModel) the pretrained model\n",
    "#         \"\"\"\n",
    "#         continuous_actions = isinstance(self.action_space, gym.spaces.Box)\n",
    "#         discrete_actions = isinstance(self.action_space, gym.spaces.Discrete)\n",
    "\n",
    "#         assert discrete_actions or continuous_actions, 'Only Discrete and Box action spaces are supported'\n",
    "\n",
    "#         # Validate the model every 10% of the total number of iteration\n",
    "#         if val_interval is None:\n",
    "#             # Prevent modulo by zero\n",
    "#             if n_epochs < 10:\n",
    "#                 val_interval = 1\n",
    "#             else:\n",
    "#                 val_interval = int(n_epochs / 10)\n",
    "\n",
    "#         with self.graph.as_default():\n",
    "#             with tf.variable_scope('pretrain', reuse=tf.AUTO_REUSE):\n",
    "#                 if continuous_actions:\n",
    "#                     obs_ph, actions_ph, deterministic_actions_ph = self._get_pretrain_placeholders()\n",
    "#                     loss = tf.reduce_mean(tf.square(actions_ph - deterministic_actions_ph))\n",
    "#                 else:\n",
    "#                     obs_ph, actions_ph, actions_logits_ph = self._get_pretrain_placeholders()\n",
    "#                     # actions_ph has a shape if (n_batch,), we reshape it to (n_batch, 1)\n",
    "#                     # so no additional changes is needed in the dataloader\n",
    "#                     actions_ph = tf.expand_dims(actions_ph, axis=1)\n",
    "#                     one_hot_actions = tf.one_hot(actions_ph, self.action_space.n)\n",
    "#                     loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "#                         logits=actions_logits_ph,\n",
    "#                         labels=tf.stop_gradient(one_hot_actions)\n",
    "#                     )\n",
    "#                     loss = tf.reduce_mean(loss)\n",
    "#                 optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=adam_epsilon)\n",
    "#                 optim_op = optimizer.minimize(loss, var_list=self.params)\n",
    "\n",
    "#             self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#         if self.verbose > 0:\n",
    "#             print(\"Pretraining with Behavior Cloning...\")\n",
    "\n",
    "#         for epoch_idx in range(int(n_epochs)):\n",
    "#             train_loss = 0.0\n",
    "#             # Full pass on the training set\n",
    "#             for _ in range(len(dataset.train_loader)):\n",
    "#                 expert_obs, expert_actions = dataset.get_next_batch('train')\n",
    "#                 feed_dict = {\n",
    "#                     obs_ph: expert_obs,\n",
    "#                     actions_ph: expert_actions,\n",
    "#                 }\n",
    "#                 train_loss_, _ = self.sess.run([loss, optim_op], feed_dict)\n",
    "#                 train_loss += train_loss_\n",
    "\n",
    "#             train_loss /= len(dataset.train_loader)\n",
    "\n",
    "#             if self.verbose > 0 and (epoch_idx + 1) % val_interval == 0:\n",
    "#                 val_loss = 0.0\n",
    "#                 # Full pass on the validation set\n",
    "#                 for _ in range(len(dataset.val_loader)):\n",
    "#                     expert_obs, expert_actions = dataset.get_next_batch('val')\n",
    "#                     val_loss_, = self.sess.run([loss], {obs_ph: expert_obs,\n",
    "#                                                         actions_ph: expert_actions})\n",
    "#                     val_loss += val_loss_\n",
    "\n",
    "#                 val_loss /= len(dataset.val_loader)\n",
    "#                 if self.verbose > 0:\n",
    "#                     print(\"==== Training progress {:.2f}% ====\".format(100 * (epoch_idx + 1) / n_epochs))\n",
    "#                     print('Epoch {}'.format(epoch_idx + 1))\n",
    "#                     print(\"Training loss: {:.6f}, Validation loss: {:.6f}\".format(train_loss, val_loss))\n",
    "#                     print()\n",
    "#             # Free memory\n",
    "#             del expert_obs, expert_actions\n",
    "#         if self.verbose > 0:\n",
    "#             print(\"Pretraining done.\")\n",
    "#         return self\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
